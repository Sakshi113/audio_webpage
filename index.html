<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>UMD GAMMA Lab Audio Processing Group</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>

    <header>
        <h1>UMD GAMMA Lab Audio Processing Group</h1>
    </header>
    <!-- Section 1 -->
    <div class="section">
        <div class="content">
            <h1><i>GAMA:</i> A Large Audio-Language Model with Advanced Audio
                Understanding and Complex Reasoning Abilities</h1>
            <div class="description">
                <p>
                    We propose GAMA, a novel Large Audio-Language Model (LALM) that is capable of responding accurately to complex questions
                    about an input audio. GAMA benefits from a mixture of encoders and synthetic data generated using a novel data generation
                      pipeline we propose. GAMA currently stands as the state-of-the-art LALM on various audio understanding, reasoning and hallucination benchmarks.

                </p>
            </div>
            <div class="buttons">
                <a href="https://arxiv.org/abs/2406.11768" target="_blank"><button>arXiv</button></a>
                <a href="https://sreyan88.github.io/gamaaudio/" target="_blank"><button>Homepage</button></a>
                <a href="https://github.com/Sreyan88/GAMA" target="_blank"><button>Code</button></a>
                <a href="https://huggingface.co/spaces/sonalkum/GAMA" target="_blank"><button>GAMA Demo</button></a>
                <a href="https://huggingface.co/spaces/sonalkum/GAMA-IT" target="_blank"><button>GAMA-IT Demo</button></a>
            </div>
        </div>
        <div class="image">
            <img src="gama.jpg" alt="Earth-like view of a fictional landmass">
        </div>
    </div>


    <div class="section">
        <div class="content">
            <h1><i>MMAU:</i> A Massive Multi-Task Audio Understanding and Reasoning Benchmark</h1>
            <div class="description">
                <p>We introduce MMAU (Massive Multi-Task Audio Understanding and Reasoning Benchmark), a comprehensive benchmark 
                    designed to evaluate Large Audio-Language Models (LALMs) on tasks that demand expert-level knowledge and complex 
                    reasoning. MMAU includes 10,000 meticulously curated audio clips paired with human-annotated natural language questions 
                    and answers, covering speech, environmental sounds, and music. The benchmark features information extraction and 
                    reasoning questions that require models to demonstrate 27 distinct skills across unique and challenging tasks. Notably,
                     even the advanced Gemini Pro v1.5 achieves only 52.97% accuracy, and the state-of-the-art open-source Qwen2-Audio achieves 52.50%,
                      underscoring significant potential for improvement.
                </p>
            </div>
            <div class="buttons">
                <a href="https://arxiv.org/" target="_blank"><button>arXiv</button></a>
                <a href="https://example.com/pdf" target="_blank"><button>PDF</button></a>
                <a href="https://example.com/code" target="_blank"><button>Code</button></a>
                <a href="https://example.com/checkpoints" target="_blank"><button>Checkpoints</button></a>
                <a href="https://example.com/demo" target="_blank"><button>Demo</button></a>
            </div>
        </div>
        <div class="image">
            <img src="mmau-hero.jpg" alt="Earth-like view of a fictional landmass">
        </div>
    </div>
    

    <!-- Section 2 -->
    <div class="section">
        <div class="content">
            <h1><i>Synthio:</i> Augmenting Small-Scale Audio Classification Datasets with Synthetic Data</h1>
            <div class="description">
                <p>We present Synthio, a novel method for generating synthetic data specifically for audio classification. 
                    Our approach first involves aligning a Text-to-Audio generation model with the target dataset through 
                    preference optimization. We then introduce an iterative prompting method with large language models (LLMs) 
                    to generate diverse and consistent audio captions, which are used to prompt the Text-to-Audio generation model 
                    for synthetic data creation. By augmenting small-scale audio classification datasets with data generated by Synthio,
                     we achieve up to a 39% performance improvement on benchmark datasets.
                </p>
            </div>
            <div class="buttons">
                <a href="https://arxiv.org/" target="_blank"><button>arXiv</button></a>
                <a href="https://example.com/pdf" target="_blank"><button>PDF</button></a>
                <a href="https://example.com/code" target="_blank"><button>Code</button></a>
                <a href="https://example.com/checkpoints" target="_blank"><button>Checkpoints</button></a>
                <a href="https://example.com/demo" target="_blank"><button>Demo</button></a>
            </div>
        </div>
        <div class="image">
            <img src="synthio.jpg" alt="Earth-like view of a fictional landmass">
        </div>
    </div>

    <!-- Section 3 -->
    <div class="section">
        <div class="content">
            <h1><i>EH-MAM:</i> Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning</h1>
            <div class="description">
                <p>We introduce EH-MAM (Easy-to-Hard adaptive Masked Acoustic Modeling), a novel self-supervised approach for speech representation learning.
                    EH-MAM enables better learning from unsupervised data by using an adaptive masking strategy that gradually increases the difficulty of the p
                    re-text SSL task and selectively reconstructing challenging regions within the speech input. EH-MAM outperforms several state-of-the-art baselines across various low-resource speech recognition and SUPERB benchmarks
                    by 5%-10%.
                </p>
            </div>
            <div class="buttons">
                <button>arXiv</button>
                <button>PDF</button>
                <button>Code</button>
                <button>Checkpoints</button>
                <button>Demo</button>
            </div>
        </div>
        <div class="image">
            <img src="eh-mam.jpg" alt="Earth-like view of a fictional landmass">
        </div>
    </div>

    <div class="section">
        <div class="content">
            <h1><i>PAT:</i> </h1>
            <div class="description">
                <p>We introduce PAT (Parameter-free Audio-Text aligner), a novel training and parameter-free method designed to 
                    boost zero-shot audio classification performance with audio-language models. PAT achieves this by improving
                     test-time audio-text alignment, enhancing representations for both modalities through mutual feedback. PAT 
                     outperforms vanilla zero-shot audio classification with significant margins of 0.42%-27.0%.
                </p>
            </div>
            <div class="buttons">
                <button>arXiv</button>
                <button>PDF</button>
                <button>Code</button>
                <button>Checkpoints</button>
                <button>Demo</button>
            </div>
        </div>
        <div class="image">
            <img src="pat.jpg" alt="Earth-like view of a fictional landmass">
        </div>
    </div>

</body>
</html>
