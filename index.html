<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>UMD GAMMA Lab Audio Processing Group</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>

    <header>
        <img src="umd_logo.png" alt="Earth-like view of a fictional landmass">
        <h1>UMD GAMMA Lab Audio Processing Group</h1>
    </header>
    <!-- Section 1 -->
    <div class="section">
        <div class="content">
            <h1><i>GAMA:</i> A Large Audio-Language Model with Advanced Audio
                Understanding and Complex Reasoning Abilities</h1>
            <div class="description">
                <p>
                    We propose GAMA, a novel Large Audio-Language Model (LALM) that is capable of responding accurately to complex questions
                    about an input audio. GAMA benefits from a mixture of encoders and synthetic data generated using a novel data generation
                      pipeline we propose. GAMA currently stands as the state-of-the-art LALM on various audio understanding, reasoning and hallucination benchmarks.

                </p>
            </div>
            <div class="buttons">
                <a href="https://arxiv.org/abs/2406.11768" target="_blank"><button>arXiv</button></a>
                <a href="https://sreyan88.github.io/gamaaudio/" target="_blank"><button>Homepage</button></a>
                <a href="https://github.com/Sreyan88/GAMA" target="_blank"><button>Code</button></a>
                <a href="https://huggingface.co/spaces/sonalkum/GAMA" target="_blank"><button>GAMA Demo</button></a>
                <a href="https://huggingface.co/spaces/sonalkum/GAMA-IT" target="_blank"><button>GAMA-IT Demo</button></a>
            </div>
        </div>
        <div class="image">
            <img src="gama.jpg" alt="Earth-like view of a fictional landmass">
        </div>
    </div>


    <div class="section">
        <div class="content">
            <h1><i>MMAU:</i> A Massive Multi-Task Audio Understanding and Reasoning Benchmark</h1>
            <div class="description">
                <p>We introduce MMAU (Massive Multi-Task Audio Understanding and Reasoning Benchmark), a comprehensive benchmark 
                    designed to evaluate Large Audio-Language Models (LALMs) on tasks that demand expert-level knowledge and complex 
                    reasoning. MMAU includes 10,000 meticulously curated audio clips paired with human-annotated natural language questions 
                    and answers, covering speech, environmental sounds, and music. The benchmark features information extraction and 
                    reasoning questions that require models to demonstrate 27 distinct skills across unique and challenging tasks. Notably,
                     even the advanced Gemini Pro v1.5 achieves only 52.97% accuracy, and the state-of-the-art open-source Qwen2-Audio achieves 52.50%,
                      underscoring significant potential for improvement.
                </p>
            </div>
            <div class="buttons">
                <a href="https://arxiv.org/" target="_blank"><button>arXiv</button></a>
                <a href="https://github.com/Sakshi113/mmau/tree/main" target="_blank"><button>Homepage</button></a>
                <a href="https://example.com/code" target="_blank"><button>Code</button></a>
                <a href="https://eval.ai/web/challenges/challenge-page/2391/overview" target="_blank"><button>EvalAI</button></a>
            </div>
        </div>
        <div class="image">
            <img src="mmau-hero.jpg" alt="Earth-like view of a fictional landmass">
        </div>
    </div>

    <div class="section">
        <div class="content">
            <h1><i>CompA:</i> Augmenting Small-Scale Audio Classification Datasets with Synthetic Data</h1>
            <div class="description">
                <p>We introduce CompA, a benchmark specifically designed to address gaps in compositional reasoning in 
                    audio-language models (ALMs). CompA includes two expert-annotated benchmarks: CompA-order, which evaluates 
                    how well an ALM understands the sequence of acoustic events, and CompA-attribute, which tests the modelâ€™s ability 
                    to associate attributes with specific sounds. Each test instance contains audio-caption pairs with the same events 
                    but in varying compositions, challenging the model to match audio accurately to captions. Using CompA, we demonstrate 
                    that current ALMs, including CLAP, struggle with complex compositional reasoning. To improve performance, we propose 
                    CompA-CLAP, a fine-tuned model that leverages compositionally-aware hard negatives and a new modular contrastive learning 
                    objective, significantly enhancing compositional reasoning capabilities across both benchmarks.
                </p>
            </div>
            <div class="buttons">
                <a href="https://arxiv.org/pdf/2310.08753" target="_blank"><button>arXiv</button></a>
                <a href=" https://sreyan88.github.io/compa_iclr/" target="_blank"><button>Homepage</button></a>
                <a href="https://github.com/Sreyan88/CompA" target="_blank"><button>Code</button></a>
            </div>
        </div>
        <div class="image">
            <img src="compa.png" alt="Earth-like view of a fictional landmass">
        </div>
    </div>
    

    <!-- Section 2 -->
    <div class="section">
        <div class="content">
            <h1><i>Synthio:</i>Augmenting Small-Scale Audio Classification Datasets with Synthetic Data</h1>
            <div class="description">
                <p>We present Synthio, a novel method for generating synthetic data specifically for audio classification. 
                    Our approach first involves aligning a Text-to-Audio generation model with the target dataset through 
                    preference optimization. We then introduce an iterative prompting method with large language models (LLMs) 
                    to generate diverse and consistent audio captions, which are used to prompt the Text-to-Audio generation model 
                    for synthetic data creation. By augmenting small-scale audio classification datasets with data generated by Synthio,
                     we achieve up to a 39% performance improvement on benchmark datasets.
                </p>
            </div>
            <div class="buttons">
                <a href="https://arxiv.org/pdf/2410.02056" target="_blank"><button>arXiv</button></a>
                <a href="https://example.com/code" target="_blank"><button>Code</button></a>
]                <a href="https://example.com/demo" target="_blank"><button>Demo</button></a>
            </div>
        </div>
        <div class="image">
            <img src="synthio.jpg" alt="Earth-like view of a fictional landmass">
        </div>
    </div>

    <!-- Section 3 -->
    <div class="section">
        <div class="content">
            <h1><i>EH-MAM:</i> Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning</h1>
            <div class="description">
                <p>We introduce EH-MAM (Easy-to-Hard adaptive Masked Acoustic Modeling), a novel self-supervised approach for speech representation learning.
                    EH-MAM enables better learning from unsupervised data by using an adaptive masking strategy that gradually increases the difficulty of the p
                    re-text SSL task and selectively reconstructing challenging regions within the speech input. EH-MAM outperforms several state-of-the-art baselines across various low-resource speech recognition and SUPERB benchmarks
                    by 5%-10%.
                </p>
            </div>
            <div class="buttons">
                <a href="https://arxiv.org/pdf/2410.13179" target="_blank"><button>arXiv</button></a>
                <a href="https://github.com/cs20s030/ehmam" target="_blank"><button>Code</button></a>
                <a href="https://drive.google.com/file/d/1Rx4MpeN1-0xjjKXx5zbJMCCvGLdVe1nr/view?usp=sharing" target="_blank"><button>Checkpoints</button></a>
            </div>
        </div>
        <div class="image">
            <img src="eh-mam.jpg" alt="Earth-like view of a fictional landmass">
        </div>
    </div>

    <div class="section">
        <div class="content">
            <h1><i>PAT:</i> Parameter-Free Audio-Text Aligner to Boost Zero-Shot Audio Classification</h1>
            <div class="description">
                <p>We introduce PAT (Parameter-free Audio-Text aligner), a novel training and parameter-free method designed to 
                    boost zero-shot audio classification performance with audio-language models. PAT achieves this by improving
                     test-time audio-text alignment, enhancing representations for both modalities through mutual feedback. PAT 
                     outperforms vanilla zero-shot audio classification with significant margins of 0.42%-27.0%.
                </p>
            </div>
            <div class="buttons">
                <a href="https://arxiv.org/pdf/2410.15062" target="_blank"><button>arXiv</button></a>
                <a href="https://github.com/cs20s030/PAT" target="_blank"><button>Code</button></a>
            </div>
        </div>
        <div class="image">
            <img src="pat.jpg" alt="Earth-like view of a fictional landmass">
        </div>
    </div>

    <div class="section">
        <div class="content">
            <h1>Do Audio-Language Models Understand Linguistic Variations?</h1>
            <div class="description">
                <p>We propose RobustCLAP, a compute-efficient technique that enhances audio-language representations to be robust to linguistic 
                    variations. We observe that existing ALMs struggle to generalize effectively to linguistically diverse textual queries. 
                    RobustCLAP addresses this challenge by reformulating the contrastive loss in CLAP architectures with a multi-view 
                    contrastive learning objective. This approach improves text-to-audio retrieval performance by 0.8%-13% across various benchmarks.
                </p>
            </div>
            <div class="buttons">
                <a href="https://arxiv.org/pdf/2410.16505" target="_blank"><button>arXiv</button></a>
                <a href="https://github.com/ramaneswaran/linguistic_robust_clap" target="_blank"><button>Code</button></a>
            </div>
        </div>
        <div class="image">
            <img src="robust.jpg" alt="Earth-like view of a fictional landmass">
        </div>
    </div>

    <div class="section">
        <div class="content">
            <h1><i>ReCLAP:</i> Improving Zero Shot Audio Classification by Describing Sounds</h1>
            <div class="description">
                <p>We present ReCLAP, a nvel approach to enhance zero-shot audio classification performance in 
                    CLAP-like Audio-Language Models. Our method first involves training a CLAP model using a unique 
                    caption augmentation technique, where audio captions are rewritten to describe individual acoustic 
                    events from an auditory perspective. To further improve zero-shot audio classification, we introduce 
                    a novel prompt augmentation strategy that generates custom prompts for each category by rephrasing labels 
                    to describe sounds associated with each category. ReCLAP achieves state-of-the-art performance on retrieval 
                    benchmarks and boosts zero-shot audio classification accuracy by 1%-18% across seven zero-shot classification benchmarks.
                </p>
            </div>
            <div class="buttons">
                <a href="https://arxiv.org/pdf/2409.09213" target="_blank"><button>arXiv</button></a>
                <a href="https://github.com/Sreyan88/ReCLAP" target="_blank"><button>Code</button></a>
                <a href="https://drive.google.com/drive/folders/1ZUf3HNo8wO2Ec6_cfQ0nc1fUknkHSP9e?usp=sharing" target="_blank"><button>Checkpoints</button></a>
            </div>
        </div>
        <div class="image">
            <img src="pat.jpg" alt="Earth-like view of a fictional landmass">
        </div>
    </div>

</body>
</html>
