<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Audio Processing in Large Audio Language Models</title>
    <link rel="stylesheet" href="style.css">
    <link rel="icon" href="favicon.ico" type="image/x-icon">
</head>
<body>

    <header>
        <div class="header-container">
            <img src="assets/umd.png" alt="UMD Logo" class="logo-left">
            <h1>Audio Processing in Large Audio Language Models</h1>
            <img src="assets/gamma_logo-2.png" alt="GAMMA Lab Logo" class="logo-right">
        </div>
    </header>

    <!-- Section 1 -->
    <!-- Introduction Section -->
<section class="intro-section">
    <div class="container">
        <h2>Our Goal</h2>
        <!-- First Paragraph: Full Width -->
        <p class="full-width">
            Audio comprehension—including speech, non-speech sounds, and music—is essential for AI agents to interact effectively with the world. Yet, research in audio processing has lagged behind other areas like language and vision, hindered by limited datasets, the need for advanced architectures, and training methods suited to the inherent complexities of audio. However, the rise of Large Language Models (LLMs) offers promising new directions, as they have shown remarkable ability to understand and reason about the world through language, pushing forward foundational audio tasks like Automatic Speech Recognition (ASR), cross-modal retrieval, and audio captioning. While essential, these tasks only scratch the surface of true, complex reasoning needed to reach intelligence levels comparable to skilled human cognition.
        </p>
        <!-- Wrapper for Image and Subsequent Paragraphs -->
        <div class="text-image-wrapper">
            <img src="assets/GAMA_hero.jpeg" alt="GAMA Hero Image" class="intro-image">
            <p>
                At GAMMA Lab, UMD, we aim to bridge this gap with a range of innovative solutions, starting with GAMA (EMNLP 2024), our large audio-language model designed for advanced audio perception and complex reasoning. GAMA is built with a specialized architecture, optimized audio encoding, and a novel alignment dataset, positioning it as a leader across benchmarks for audio understanding, reasoning, and hallucination reduction. Good representations are key to advancing perception and GAMA’s development builds on our past achievements, such as MAST and SLICER (ICASSP 2023) and EH-MAM (EMNLP 2024), which pioneered approaches for learning strong audio representations from unlabeled data. Complementing this, we introduced ReCLAP, a state-of-the-art audio-language encoder, and CompA, one of the first projects to tackle compositional reasoning in audio-language models—a critical challenge given audio’s inherently compositional nature.
            </p>
            <p>
                Looking forward, we envision LALMs becoming integral to daily life, capable of conversational speech QA, information-extraction-based QA, and addressing knowledge-driven questions about diverse audio inputs. We aim to extend GAMA to process longer audio inputs beyond 30 seconds, and ultimately, to interpret multimodal content by integrating visual input, enabling complex question-answering over long video content. Achieving these ambitious goals requires both advanced data and architectures. Synthio, our latest synthetic data generation framework, supports this mission by generating data for complex audio understanding. Progress must also be measurable, so we’re dedicated to establishing comprehensive benchmarks. Our recent release, MMAU, rigorously tests LALMs on real-world tasks, and we plan to roll out additional benchmarks focusing on advanced reasoning over long and multi-audio scenarios.
            </p>
            <p>
                Together with open-source resources, advanced audio-language models, encoders, and synthetic data frameworks, we are accelerating audio-language intelligence to meet the demands of tomorrow’s AI applications.
            </p>
        </div>
    </div>
        <div class="container">
            <h2>Our Team</h2>
        </div>
        <div class="advisor-section">
            <div class="advisor-member">
                <a href="https://www.cs.umd.edu/people/dmanocha">
                <img src="people/dinesh.jpg" alt="Advisor's Name" class="advisor-headshot">
                <p class="advisor-name">Prof. Dinesh Manocha</p>
            </a>
            </div>
        </div>
        <div class="team-wrapper">
            <div class="container">
            <!-- Repeat this block for each person -->
                <div class="team-member">
                    <a href="https://sakshi113.github.io/">
                        <img src="people/sakshi-sf2.jpeg" alt="Person's Name2 Headshot" class="team-headshot">
                        <p class="team-name">Sakshi</p>
                    </a>
                </div>
                <div class="team-member">
                    <a href="https://sreyan88.github.io/research/">
                        <img src="people/sreyan-2.jpg" alt="Person's Name2 Headshot" class="team-headshot">
                        <p class="team-name">Sreyan Ghosh</p>
                    </a>
                </div>
                <div class="team-member">
                    <a href="https://sonalkum.github.io">
                        <img src="people/sonal.png" alt="Person's Name2 Headshot" class="team-headshot">
                        <p class="team-name">Sonal Kumar</p>
                    </a>
                </div>
                <div class="team-member">
                    <a href="https://ramaneswaran.github.io/">
                        <img src="people/raman.jpg" alt="Person's Name2 Headshot" class="team-headshot">
                        <p class="team-name">Ramaneswaran S.</p>
                    </a>
                </div>
                <div class="team-member">
                    <a href="https://cs20s030.github.io/">
                        <img src="people/ashish.jpeg" alt="Person's Name Headshot" class="team-headshot">
                        <p class="team-name">Ashish Seth</p>
                    </a>
                </div>        
                               
                <div class="team-member">
                    <a href="https://utkarsh4430.github.io/">
                        <img src="people/utkarsh.jpeg" alt="Person's Name2 Headshot" class="team-headshot">
                        <p class="team-name">Utkarsh Tyagi</p>
                    </a>
                </div>
            </div>
        </div>
    </section>

    <!-- Projects Sections -->
    <section class="intro-section">
        <div class="container">
            <h2>Publications</h2>
        </div>
    </section>
    <section class="project-section">
        <div class="container">
            <div class="text-content">
                <h2>GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities</h2>
                <p>
                    We propose GAMA, a novel Large Audio-Language Model (LALM) that is capable of responding accurately to complex questions about an input audio. GAMA benefits from a mixture of encoders and synthetic data generated using a novel data generation pipeline we propose. GAMA currently stands as the state-of-the-art LALM on various audio understanding, reasoning, and hallucination benchmarks.
                </p>
                <div class="buttons">
                    <a href="https://arxiv.org/abs/2406.11768" target="_blank" class="btn">arXiv</a>
                    <a href="https://sreyan88.github.io/gamaaudio/" target="_blank" class="btn">Homepage</a>
                    <a href="https://github.com/Sreyan88/GAMA" target="_blank" class="btn">Code</a>
                    <a href="https://huggingface.co/spaces/sonalkum/GAMA" target="_blank" class="btn">GAMA Demo</a>
                    <a href="https://huggingface.co/spaces/sonalkum/GAMA-IT" target="_blank" class="btn">GAMA-IT Demo</a>
                </div>
            </div>
            <div class="image-content">
                <img src="assets/gama-hero.jpg" alt="GAMA Project Image">
            </div>
        </div>
    </section>
    <section class="project-section">
        <div class="container">
            <div class="text-content">
                <h2>MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark</h2>
                <p>
                    We introduce MMAU (Massive Multi-Task Audio Understanding and Reasoning Benchmark), a comprehensive benchmark 
                    designed to evaluate Large Audio-Language Models (LALMs) on tasks that demand expert-level knowledge and complex 
                    reasoning. MMAU includes 10,000 meticulously curated audio clips paired with human-annotated natural language questions 
                    and answers, covering speech, environmental sounds, and music. The benchmark features information extraction and 
                    reasoning questions that require models to demonstrate 27 distinct skills across unique and challenging tasks. Notably,
                     even the advanced Gemini Pro v1.5 achieves only 52.97% accuracy, and the state-of-the-art open-source Qwen2-Audio achieves 52.50%,
                      underscoring significant potential for improvement.                </p>
                <div class="buttons">
                    <a href="https://arxiv.org/pdf/2410.19168" target="_blank" class="btn">arXiv</a>
                    <a href="https://sakshi113.github.io/mmau_homepage/" target="_blank" class="btn">Homepage</a>
                    <a href="https://github.com/Sakshi113/mmau/tree/main" target="_blank" class="btn">Code</a>
                    <a href="https://eval.ai/web/challenges/challenge-page/2391/overview" target="_blank" class="btn">EvalAI</a>
                </div>
            </div>
            <div class="image-content">
                <img src="assets/mmau-hero.jpg" alt="MMAU Project Image">
            </div>
        </div>
    </section>
    <section class="project-section">
        <div class="container">
            <div class="text-content">
                <h2>CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models</h2>
                <p>We introduce CompA, a benchmark specifically designed to address gaps in compositional reasoning in 
                    audio-language models (ALMs). CompA includes two expert-annotated benchmarks: CompA-order, which evaluates 
                    how well an ALM understands the sequence of acoustic events, and CompA-attribute, which tests the model’s ability 
                    to associate attributes with specific sounds. Each test instance contains audio-caption pairs with the same events 
                    but in varying compositions, challenging the model to match audio accurately to captions. Using CompA, we demonstrate 
                    that current ALMs, including CLAP, struggle with complex compositional reasoning. To improve performance, we propose 
                    CompA-CLAP, a fine-tuned model that leverages compositionally-aware hard negatives and a new modular contrastive learning 
                    objective, significantly enhancing compositional reasoning capabilities across both benchmarks.</p>
                <div class="buttons">
                    <a href="https://arxiv.org/pdf/2310.08753" target="_blank" class="btn">arXiv</a>
                    <a href="https://sreyan88.github.io/compa_iclr/" target="_blank" class="btn">Homepage</a>
                    <a href="https://github.com/Sreyan88/CompA" target="_blank" class="btn">Code</a>
                </div>
            </div>
            <div class="image-content">
                <img src="assets/compa.png" alt="CompA Project Image">
            </div>
        </div>
    </section>
    <section class="project-section">
        <div class="container">
            <div class="text-content">
                <h2>Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data</h2>
                <p>We present Synthio, a novel method for generating synthetic data specifically for audio classification. 
                    Our approach first involves aligning a Text-to-Audio generation model with the target dataset through 
                    preference optimization. We then introduce an iterative prompting method with large language models (LLMs) 
                    to generate diverse and consistent audio captions, which are used to prompt the Text-to-Audio generation model 
                    for synthetic data creation. By augmenting small-scale audio classification datasets with data generated by Synthio,
                     we achieve up to a 39% performance improvement on benchmark datasets.</p>
                <div class="buttons">
                    <a href="https://arxiv.org/pdf/2410.02056" target="_blank" class="btn">arXiv</a>
                    <a href="https://github.com/Sreyan88/Synthio" target="_blank" class="btn">Code</a>
                    <a href="https://huggingface.co/spaces/sonalkum/synthio-stable-audio-open" target="_blank" class="btn">Demo</a>
                </div>
            </div>
            <div class="image-content">
                <img src="assets/synthio.jpg" alt="CompA Project Image">
            </div>
        </div>
    </section>
    <section class="project-section">
        <div class="container">
            <div class="text-content">
                <h2>EH-MAM: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning</h2>
                <p>We introduce EH-MAM (Easy-to-Hard adaptive Masked Acoustic Modeling), a novel self-supervised approach for speech representation learning.
                    EH-MAM enables better learning from unsupervised data by using an adaptive masking strategy that gradually increases the difficulty of the p
                    re-text SSL task and selectively reconstructing challenging regions within the speech input. EH-MAM outperforms several state-of-the-art baselines across various low-resource speech recognition and SUPERB benchmarks
                    by 5%-10%.</p>
                <div class="buttons">
                    <a href="https://arxiv.org/pdf/2410.13179" target="_blank" class="btn">arXiv</a>
                    <a href="https://github.com/cs20s030/ehmam" target="_blank" class="btn">Code</a>
                    <a href="https://drive.google.com/file/d/1Rx4MpeN1-0xjjKXx5zbJMCCvGLdVe1nr/view?usp=sharing" target="_blank" class="btn">Checkpoint</a>
                </div>
            </div>
            <div class="image-content">
                <img src="assets/eh-mam.jpg" alt="CompA Project Image">
            </div>
        </div>
    </section>
    <section class="project-section">
        <div class="container">
            <div class="text-content">
                <h2>PAT: Parameter-Free Audio-Text Aligner to Boost Zero-Shot Audio Classification</h2>
                <p>We introduce PAT (Parameter-free Audio-Text aligner), a novel training and parameter-free method designed to 
                    boost zero-shot audio classification performance with audio-language models. PAT achieves this by improving
                     test-time audio-text alignment, enhancing representations for both modalities through mutual feedback. PAT 
                     outperforms vanilla zero-shot audio classification with significant margins of 0.42%-27.0%.</p>
                <div class="buttons">
                    <a href="https://arxiv.org/pdf/2410.15062" target="_blank" class="btn">arXiv</a>
                    <a href="https://github.com/cs20s030/PAT" target="_blank" class="btn">Code</a>
                </div>
            </div>
            <div class="image-content">
                <img src="assets/pat.jpg" alt="CompA Project Image">
            </div>
        </div>
    </section>
    <section class="project-section">
        <div class="container">
            <div class="text-content">
                <h2>Do Audio-Language Models Understand Linguistic Variations?</h2>
                <p>We propose RobustCLAP, a compute-efficient technique that enhances audio-language representations to be robust to linguistic 
                    variations. We observe that existing ALMs struggle to generalize effectively to linguistically diverse textual queries. 
                    RobustCLAP addresses this challenge by reformulating the contrastive loss in CLAP architectures with a multi-view 
                    contrastive learning objective. This approach improves text-to-audio retrieval performance by 0.8%-13% across various benchmarks.</p>
                <div class="buttons">
                    <a href="https://arxiv.org/pdf/2410.16505" target="_blank" class="btn">arXiv</a>
                    <a href="https://github.com/ramaneswaran/linguistic_robust_clap" target="_blank" class="btn">Code</a>
                </div>
            </div>
            <div class="image-content">
                <img src="assets/robust.jpg" alt="CompA Project Image">
            </div>
        </div>
    </section>
    <section class="project-section">
        <div class="container">
            <div class="text-content">
                <h2>ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds</h2>
                <p>We present ReCLAP, a nvel approach to enhance zero-shot audio classification performance in 
                    CLAP-like Audio-Language Models. Our method first involves training a CLAP model using a unique 
                    caption augmentation technique, where audio captions are rewritten to describe individual acoustic 
                    events from an auditory perspective. To further improve zero-shot audio classification, we introduce 
                    a novel prompt augmentation strategy that generates custom prompts for each category by rephrasing labels 
                    to describe sounds associated with each category. ReCLAP achieves state-of-the-art performance on retrieval 
                    benchmarks and boosts zero-shot audio classification accuracy by 1%-18% across seven zero-shot classification benchmarks.</p>
                <div class="buttons">
                    <a href="https://arxiv.org/pdf/2409.09213" target="_blank" class="btn">arXiv</a>
                    <a href="https://github.com/Sreyan88/ReCLA" target="_blank" class="btn">Code</a>
                    <a href="https://drive.google.com/drive/folders/1ZUf3HNo8wO2Ec6_cfQ0nc1fUknkHSP9e?usp=sharing" target="_blank", class="btn">Checkpoints</a>
                </div>
            </div>
            <div class="image-content">
                <img src="assets/reclap.jpg" alt="CompA Project Image" style="height: 450px; width: 400px; margin-left: 150px;">
            </div>
        </div>
    </section>


    <!-- Repeat the above project section for each project, updating content and links accordingly -->

    <!-- Footer -->
    <footer>
        <p>&copy; 2023 UMD GAMMA Lab Audio Processing Group</p>
    </footer>

    <!-- Optional JavaScript for Interactive Effects -->
    <script src="script.js"></script>
</body>
</html>
