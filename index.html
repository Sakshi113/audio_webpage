<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>UMD GAMMA Lab Audio Processing Group</title>
    <link rel="stylesheet" href="style.css">
    <link rel="icon" href="favicon.ico" type="image/x-icon">
</head>
<body>

    <header>
        <h1>UMD GAMMA Lab Audio Processing Group</h1>
    </header>

    <!-- Section 1 -->
    <section class="intro-section">
        <div class="container">
            <div class="text-content">
                <h2>Our Goal</h2>
                <p>
                    Audio comprehension—including speech, non-speech sounds, and music—is essential for AI agents to interact effectively with the world. Yet, research in audio processing has lagged behind other areas like language and vision, hindered by limited datasets, the need for advanced architectures, and training methods suited to the inherent complexities of audio. However, the rise of Large Language Models (LLMs) offers promising new directions, as they have shown remarkable ability to understand and reason about the world through language, pushing forward foundational audio tasks like Automatic Speech Recognition (ASR), cross-modal retrieval, and audio captioning. While essential, these tasks only scratch the surface of true, complex reasoning needed to reach intelligence levels comparable to skilled human cognition.
                </p>
                
                <p>
                    At GAMMA Lab, UMD, we aim to bridge this gap with a range of innovative solutions, starting with GAMA (EMNLP 2024), our large audio-language model designed for advanced audio perception and complex reasoning. GAMA is built with a specialized architecture, optimized audio encoding, and a novel alignment dataset, positioning it as a leader across benchmarks for audio understanding, reasoning, and hallucination reduction. Good representations are key to advancing perception and GAMA’s development builds on our past achievements, such as MAST and SLICER (ICASSP 2023) and EH-MAM (EMNLP 2024), which pioneered approaches for learning strong audio representations from unlabeled data. Complementing this, we introduced ReCLAP, a state-of-the-art audio-language encoder, and CompA, one of the first projects to tackle compositional reasoning in audio-language models—a critical challenge given audio’s inherently compositional nature.
                </p>
                <p>
                    Looking forward, we envision LALMs becoming integral to daily life, capable of conversational speech QA, information-extraction-based QA, and addressing knowledge-driven questions about diverse audio inputs. We aim to extend GAMA to process longer audio inputs beyond 30 seconds, and ultimately, to interpret multimodal content by integrating visual input, enabling complex question-answering over long video content. Achieving these ambitious goals requires both advanced data and architectures. Synthio, our latest synthetic data generation framework, supports this mission by generating data for complex audio understanding. Progress must also be measurable, so we’re dedicated to establishing comprehensive benchmarks. Our recent release, MMAU, rigorously tests LALMs on real-world tasks, and we plan to roll out additional benchmarks focusing on advanced reasoning over long and multi-audio scenarios.
                </p>
                <p>
                    Together with open-source resources, advanced audio-language models, encoders, and synthetic data frameworks, we are accelerating audio-language intelligence to meet the demands of tomorrow’s AI applications.
                </p>
            </div>
            <div class="image-content">
                <img src="GAMA_hero.jpeg" alt="GAMA Hero Image">
            </div>
        </div>
    </section>

    <!-- Projects Sections -->
    <!-- Repeat this section for each project -->
    <section class="project-section">
        <div class="container">
            <div class="text-content">
                <h2>GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities</h2>
                <p>
                    We propose GAMA, a novel Large Audio-Language Model (LALM) that is capable of responding accurately to complex questions about an input audio. GAMA benefits from a mixture of encoders and synthetic data generated using a novel data generation pipeline we propose. GAMA currently stands as the state-of-the-art LALM on various audio understanding, reasoning, and hallucination benchmarks.
                </p>
                <div class="buttons">
                    <a href="https://arxiv.org/abs/2406.11768" target="_blank" class="btn">arXiv</a>
                    <a href="https://sreyan88.github.io/gamaaudio/" target="_blank" class="btn">Homepage</a>
                    <a href="https://github.com/Sreyan88/GAMA" target="_blank" class="btn">Code</a>
                    <a href="https://huggingface.co/spaces/sonalkum/GAMA" target="_blank" class="btn">GAMA Demo</a>
                    <a href="https://huggingface.co/spaces/sonalkum/GAMA-IT" target="_blank" class="btn">GAMA-IT Demo</a>
                </div>
            </div>
            <div class="image-content">
                <img src="gama-hero.jpg" alt="GAMA Project Image">
            </div>
        </div>
    </section>
    <section class="project-section">
        <div class="container">
            <div class="text-content">
                <h2>MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark</h2>
                <p>
                    We introduce MMAU (Massive Multi-Task Audio Understanding and Reasoning Benchmark), a comprehensive benchmark 
                    designed to evaluate Large Audio-Language Models (LALMs) on tasks that demand expert-level knowledge and complex 
                    reasoning. MMAU includes 10,000 meticulously curated audio clips paired with human-annotated natural language questions 
                    and answers, covering speech, environmental sounds, and music. The benchmark features information extraction and 
                    reasoning questions that require models to demonstrate 27 distinct skills across unique and challenging tasks. Notably,
                     even the advanced Gemini Pro v1.5 achieves only 52.97% accuracy, and the state-of-the-art open-source Qwen2-Audio achieves 52.50%,
                      underscoring significant potential for improvement.                </p>
                <div class="buttons">
                    <a href="https://arxiv.org/pdf/2410.19168" target="_blank" class="btn">arXiv</a>
                    <a href="https://sakshi113.github.io/mmau_homepage/" target="_blank" class="btn">Homepage</a>
                    <a href="https://github.com/Sakshi113/mmau/tree/main" target="_blank" class="btn">Code</a>
                    <a href="https://eval.ai/web/challenges/challenge-page/2391/overview" target="_blank" class="btn">EvalAI</a>
                </div>
            </div>
            <div class="image-content">
                <img src="mmau-hero.jpg" alt="MMAU Project Image">
            </div>
        </div>
    </section>


    <!-- Repeat the above project section for each project, updating content and links accordingly -->

    <!-- Footer -->
    <footer>
        <p>&copy; 2023 UMD GAMMA Lab Audio Processing Group</p>
    </footer>

    <!-- Optional JavaScript for Interactive Effects -->
    <script src="script.js"></script>
</body>
</html>
